{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a95d8a2",
   "metadata": {},
   "source": [
    "# Implementing basic operations of spaCy, NLTK, regex libraries \n",
    "\n",
    "Using sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b0758f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/victoria/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad5b08b",
   "metadata": {},
   "source": [
    "## 1. Loading the sample data in Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b71ea47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        id               updated  \\\n",
      "0  http://arxiv.org/abs/astro-ph/0407044v1  2004-07-02T10:17:39Z   \n",
      "1  http://arxiv.org/abs/astro-ph/0410439v1  2004-10-19T14:47:51Z   \n",
      "2  http://arxiv.org/abs/astro-ph/0411574v3  2011-01-05T18:55:32Z   \n",
      "3  http://arxiv.org/abs/astro-ph/0504497v1  2005-04-22T12:39:07Z   \n",
      "4   http://arxiv.org/abs/physics/0510224v1  2005-10-25T15:36:07Z   \n",
      "\n",
      "              published                                              title  \\\n",
      "0  2004-07-02T10:17:39Z  Muon Track Reconstruction and Data Selection T...   \n",
      "1  2004-10-19T14:47:51Z                   An update on the SCUBA-2 project   \n",
      "2  2004-11-19T15:00:42Z  Feasibility study of a Laue lens for hard X-ra...   \n",
      "3  2005-04-22T12:39:07Z  Search for Extra-Terrestrial planets: The DARW...   \n",
      "4  2005-10-25T15:36:07Z  Wavefront sensor based on varying transmission...   \n",
      "\n",
      "                                             summary  \\\n",
      "0  The Antarctic Muon And Neutrino Detector Array...   \n",
      "1  SCUBA-2, which replaces SCUBA (the Submillimet...   \n",
      "2  We report on the feasibility study of a Laue l...   \n",
      "3  The DARWIN mission is an Infrared free flying ...   \n",
      "4  The use of Wavefront Sensors (WFS) is nowadays...   \n",
      "\n",
      "                                              author  \\\n",
      "0  [{'name': 'The AMANDA Collaboration'}, {'name'...   \n",
      "1  [{'name': 'Michael Audley', 'affiliation': 'UK...   \n",
      "2  [{'name': 'A. Pisa', 'affiliation': 'Universit...   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                          doi  \\\n",
      "0  10.1016/j.nima.2004.01.065   \n",
      "1           10.1117/12.551259   \n",
      "2           10.1117/12.563052   \n",
      "3                         NaN   \n",
      "4   10.1080/09500340500073495   \n",
      "\n",
      "                                        link_related  \\\n",
      "0  [http://dx.doi.org/10.1016/j.nima.2004.01.065,...   \n",
      "1  [http://dx.doi.org/10.1117/12.551259, http://a...   \n",
      "2  [http://dx.doi.org/10.1117/12.563052, http://a...   \n",
      "3            http://arxiv.org/pdf/astro-ph/0504497v1   \n",
      "4  [http://dx.doi.org/10.1080/09500340500073495, ...   \n",
      "\n",
      "                                             comment  \\\n",
      "0   40 pages, 16 Postscript figures, uses elsart.sty   \n",
      "1  16 pages, 14 figures, Invited talk at SPIE Gla...   \n",
      "2  10 pages, corrected Fig. 1b and Fig. 2, which ...   \n",
      "3  PhD thesis 2004, Karl Franzens Univ. Graz, 177...   \n",
      "4                                2 tables, 6 figures   \n",
      "\n",
      "                           journal_ref  \\\n",
      "0  Nucl.Instrum.Meth.A524:169-194,2004   \n",
      "1                                  NaN   \n",
      "2          SPIE Proc., 5536, 39 (2004)   \n",
      "3                                  NaN   \n",
      "4         J.Mod.Opt. 52:1917-1931,2005   \n",
      "\n",
      "                            link_alternate primary_category  \\\n",
      "0  http://arxiv.org/abs/astro-ph/0407044v1         astro-ph   \n",
      "1  http://arxiv.org/abs/astro-ph/0410439v1         astro-ph   \n",
      "2  http://arxiv.org/abs/astro-ph/0411574v3         astro-ph   \n",
      "3  http://arxiv.org/abs/astro-ph/0504497v1         astro-ph   \n",
      "4   http://arxiv.org/abs/physics/0510224v1   physics.optics   \n",
      "\n",
      "                                  category       author.name  \\\n",
      "0                  [astro-ph, astro-ph.IM]               NaN   \n",
      "1                  [astro-ph, astro-ph.IM]               NaN   \n",
      "2                  [astro-ph, astro-ph.IM]               NaN   \n",
      "3     [astro-ph, astro-ph.EP, astro-ph.IM]  Lisa Kaltenegger   \n",
      "4  [physics.optics, astro-ph, astro-ph.IM]  Francois Henault   \n",
      "\n",
      "  author.affiliation  \n",
      "0                NaN  \n",
      "1                NaN  \n",
      "2                NaN  \n",
      "3                NaN  \n",
      "4                NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load directly from file\n",
    "# df = pd.read_json(\"abstracts_smaple.json\")\n",
    "\n",
    "def read_and_normalize_json(path):\n",
    "    data = pd.read_json(path)\n",
    "    return pd.json_normalize(data.to_dict(orient=\"records\"))\n",
    "\n",
    "df = read_and_normalize_json(\"abstracts_smaple.json\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05fc7e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                           id               updated  \\\n",
      "0    http://arxiv.org/abs/astro-ph/0407044v1  2004-07-02T10:17:39Z   \n",
      "1    http://arxiv.org/abs/astro-ph/0410439v1  2004-10-19T14:47:51Z   \n",
      "2    http://arxiv.org/abs/astro-ph/0411574v3  2011-01-05T18:55:32Z   \n",
      "3    http://arxiv.org/abs/astro-ph/0504497v1  2005-04-22T12:39:07Z   \n",
      "4     http://arxiv.org/abs/physics/0510224v1  2005-10-25T15:36:07Z   \n",
      "..                                       ...                   ...   \n",
      "995         http://arxiv.org/abs/0912.0014v1  2009-11-30T21:38:46Z   \n",
      "996         http://arxiv.org/abs/0912.0076v1  2009-12-01T05:41:19Z   \n",
      "997         http://arxiv.org/abs/0912.0093v1  2009-12-01T08:01:03Z   \n",
      "998         http://arxiv.org/abs/0912.0077v1  2009-12-01T11:01:06Z   \n",
      "999         http://arxiv.org/abs/0912.0143v1  2009-12-01T12:51:46Z   \n",
      "\n",
      "                published                                              title  \\\n",
      "0    2004-07-02T10:17:39Z  Muon Track Reconstruction and Data Selection T...   \n",
      "1    2004-10-19T14:47:51Z                   An update on the SCUBA-2 project   \n",
      "2    2004-11-19T15:00:42Z  Feasibility study of a Laue lens for hard X-ra...   \n",
      "3    2005-04-22T12:39:07Z  Search for Extra-Terrestrial planets: The DARW...   \n",
      "4    2005-10-25T15:36:07Z  Wavefront sensor based on varying transmission...   \n",
      "..                    ...                                                ...   \n",
      "995  2009-11-30T21:38:46Z  Evaluating the Calorimeter Model with Broadban...   \n",
      "996  2009-12-01T05:41:19Z  Automation of PRL's Astronomical Optical Polar...   \n",
      "997  2009-12-01T08:01:03Z  Apertif - the focal-plane array system for the...   \n",
      "998  2009-12-01T11:01:06Z                         INTEGRAL - a status report   \n",
      "999  2009-12-01T12:51:46Z  Neutron Capture Cross Sections for the Weak s ...   \n",
      "\n",
      "                                               summary  \\\n",
      "0    The Antarctic Muon And Neutrino Detector Array...   \n",
      "1    SCUBA-2, which replaces SCUBA (the Submillimet...   \n",
      "2    We report on the feasibility study of a Laue l...   \n",
      "3    The DARWIN mission is an Infrared free flying ...   \n",
      "4    The use of Wavefront Sensors (WFS) is nowadays...   \n",
      "..                                                 ...   \n",
      "995  Although the relationship between the far-infr...   \n",
      "996  Physical Research Laboratory's (PRL) Optical P...   \n",
      "997  We describe a focal plane array (FPA) system, ...   \n",
      "998  The ESA gamma-ray observatory INTEGRAL, launch...   \n",
      "999  In past decades a lot of progress has been mad...   \n",
      "\n",
      "                                                author  \\\n",
      "0    [{'name': 'The AMANDA Collaboration'}, {'name'...   \n",
      "1    [{'name': 'Michael Audley', 'affiliation': 'UK...   \n",
      "2    [{'name': 'A. Pisa', 'affiliation': 'Universit...   \n",
      "3                                                  NaN   \n",
      "4                                                  NaN   \n",
      "..                                                 ...   \n",
      "995  [{'name': 'Peter K. G. Williams', 'affiliation...   \n",
      "996  [{'name': 'S. Ganesh'}, {'name': 'U. C. Joshi'...   \n",
      "997  [{'name': 'Tom Oosterloo', 'affiliation': ['Ne...   \n",
      "998                                                NaN   \n",
      "999  [{'name': 'M. Heil'}, {'name': 'A. Juseviciute...   \n",
      "\n",
      "                              doi  \\\n",
      "0      10.1016/j.nima.2004.01.065   \n",
      "1               10.1117/12.551259   \n",
      "2               10.1117/12.563052   \n",
      "3                             NaN   \n",
      "4       10.1080/09500340500073495   \n",
      "..                            ...   \n",
      "995  10.1088/0004-637X/710/2/1462   \n",
      "996                           NaN   \n",
      "997                           NaN   \n",
      "998                           NaN   \n",
      "999               10.1071/AS08064   \n",
      "\n",
      "                                          link_related  \\\n",
      "0    [http://dx.doi.org/10.1016/j.nima.2004.01.065,...   \n",
      "1    [http://dx.doi.org/10.1117/12.551259, http://a...   \n",
      "2    [http://dx.doi.org/10.1117/12.563052, http://a...   \n",
      "3              http://arxiv.org/pdf/astro-ph/0504497v1   \n",
      "4    [http://dx.doi.org/10.1080/09500340500073495, ...   \n",
      "..                                                 ...   \n",
      "995  [http://dx.doi.org/10.1088/0004-637X/710/2/146...   \n",
      "996                   http://arxiv.org/pdf/0912.0076v1   \n",
      "997                   http://arxiv.org/pdf/0912.0093v1   \n",
      "998                   http://arxiv.org/pdf/0912.0077v1   \n",
      "999  [http://dx.doi.org/10.1071/AS08064, http://arx...   \n",
      "\n",
      "                                               comment  \\\n",
      "0     40 pages, 16 Postscript figures, uses elsart.sty   \n",
      "1    16 pages, 14 figures, Invited talk at SPIE Gla...   \n",
      "2    10 pages, corrected Fig. 1b and Fig. 2, which ...   \n",
      "3    PhD thesis 2004, Karl Franzens Univ. Graz, 177...   \n",
      "4                                  2 tables, 6 figures   \n",
      "..                                                 ...   \n",
      "995                 44 pages, 15 figures, ApJ accepted   \n",
      "996  16 pages, 8 figures, Physical Research Laborat...   \n",
      "997  Presented at Widefield Science and Technology ...   \n",
      "998  8 pages, invited paper, Proc. of Workshop \"The...   \n",
      "999                                                NaN   \n",
      "\n",
      "                             journal_ref  \\\n",
      "0    Nucl.Instrum.Meth.A524:169-194,2004   \n",
      "1                                    NaN   \n",
      "2            SPIE Proc., 5536, 39 (2004)   \n",
      "3                                    NaN   \n",
      "4           J.Mod.Opt. 52:1917-1931,2005   \n",
      "..                                   ...   \n",
      "995       Astrophys.J.710:1462-1479,2010   \n",
      "996                                  NaN   \n",
      "997                                  NaN   \n",
      "998          PoS extremesky2009:101,2009   \n",
      "999                                  NaN   \n",
      "\n",
      "                              link_alternate primary_category  \\\n",
      "0    http://arxiv.org/abs/astro-ph/0407044v1         astro-ph   \n",
      "1    http://arxiv.org/abs/astro-ph/0410439v1         astro-ph   \n",
      "2    http://arxiv.org/abs/astro-ph/0411574v3         astro-ph   \n",
      "3    http://arxiv.org/abs/astro-ph/0504497v1         astro-ph   \n",
      "4     http://arxiv.org/abs/physics/0510224v1   physics.optics   \n",
      "..                                       ...              ...   \n",
      "995         http://arxiv.org/abs/0912.0014v1      astro-ph.CO   \n",
      "996         http://arxiv.org/abs/0912.0076v1      astro-ph.IM   \n",
      "997         http://arxiv.org/abs/0912.0093v1      astro-ph.IM   \n",
      "998         http://arxiv.org/abs/0912.0077v1      astro-ph.HE   \n",
      "999         http://arxiv.org/abs/0912.0143v1      astro-ph.IM   \n",
      "\n",
      "                                    category        author.name  \\\n",
      "0                    [astro-ph, astro-ph.IM]                NaN   \n",
      "1                    [astro-ph, astro-ph.IM]                NaN   \n",
      "2                    [astro-ph, astro-ph.IM]                NaN   \n",
      "3       [astro-ph, astro-ph.EP, astro-ph.IM]   Lisa Kaltenegger   \n",
      "4    [physics.optics, astro-ph, astro-ph.IM]   Francois Henault   \n",
      "..                                       ...                ...   \n",
      "995               [astro-ph.CO, astro-ph.IM]                NaN   \n",
      "996                            [astro-ph.IM]                NaN   \n",
      "997               [astro-ph.IM, astro-ph.CO]                NaN   \n",
      "998               [astro-ph.HE, astro-ph.IM]  Christoph Winkler   \n",
      "999      [astro-ph.IM, astro-ph.SR, nucl-ex]                NaN   \n",
      "\n",
      "    author.affiliation  \n",
      "0                  NaN  \n",
      "1                  NaN  \n",
      "2                  NaN  \n",
      "3                  NaN  \n",
      "4                  NaN  \n",
      "..                 ...  \n",
      "995                NaN  \n",
      "996                NaN  \n",
      "997                NaN  \n",
      "998                NaN  \n",
      "999                NaN  \n",
      "\n",
      "[1000 rows x 15 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51631bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muon Track Reconstruction and Data Selection Techniques in AMANDA\n"
     ]
    }
   ],
   "source": [
    "print(df.title[0])   #accessing titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b404d26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Antarctic Muon And Neutrino Detector Array (AMANDA) is a high-energy\n",
      "neutrino telescope operating at the geographic South Pole. It is a lattice of\n",
      "photo-multiplier tubes buried deep in the polar ice between 1500m and 2000m.\n",
      "The primary goal of this detector is to discover astrophysical sources of high\n",
      "energy neutrinos. A high-energy muon neutrino coming through the earth from the\n",
      "Northern Hemisphere can be identified by the secondary muon moving upward\n",
      "through the detector. The muon tracks are reconstructed with a maximum\n",
      "likelihood method. It models the arrival times and amplitudes of Cherenkov\n",
      "photons registered by the photo-multipliers. This paper describes the different\n",
      "methods of reconstruction, which have been successfully implemented within\n",
      "AMANDA. Strategies for optimizing the reconstruction performance and rejecting\n",
      "background are presented. For a typical analysis procedure the direction of\n",
      "tracks are reconstructed with about 2 degree accuracy.\n"
     ]
    }
   ],
   "source": [
    "print(df.summary[0])   #accessing abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d491617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Antarctic Muon And Neutrino Detector Array (AMANDA) is a high-energy\n",
      "neutrino telescope operating at the geographic South Pole. It is a lattice of\n",
      "photo-multiplier tubes buried deep in the polar ice between 1500m and 2000m.\n",
      "The primary goal of this detector is to discover astrophysical sources of high\n",
      "energy neutrinos. A high-energy muon neutrino coming through the earth from the\n",
      "Northern Hemisphere can be identified by the secondary muon moving upward\n",
      "through the detector. The muon tracks are reconstructed with a maximum\n",
      "likelihood method. It models the arrival times and amplitudes of Cherenkov\n",
      "photons registered by the photo-multipliers. This paper describes the different\n",
      "methods of reconstruction, which have been successfully implemented within\n",
      "AMANDA. Strategies for optimizing the reconstruction performance and rejecting\n",
      "background are presented. For a typical analysis procedure the direction of\n",
      "tracks are reconstructed with about 2 degree accuracy.\n",
      "Muon Track Reconstruction and Data Selection Techniques in AMANDA\n"
     ]
    }
   ],
   "source": [
    "#load all abstracts in the list:\n",
    "texts = df.summary.tolist()\n",
    "#load all titles in the list:\n",
    "titles = df.title.tolist()\n",
    "print(texts[0])\n",
    "print(titles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c78e0f",
   "metadata": {},
   "source": [
    "## 1.a. Basic cleaning with regex\n",
    "Remove numbers, special characters, multiple spaces.\n",
    "\n",
    "- re - это питоновский модуль для работы с решулярными выражениями (regular expressions)\n",
    "- re.sub(pattern, replacement, string) заменяет все pattern-ы на replacement в строке string\n",
    "\n",
    "Про регулярные выражения:\n",
    "- [abc] позволяет выбрать что-то, что совпадает с одним из символов внутри квардратных скобок, здесь: a,b или c\n",
    "- ^ - это \"не\". Т.е., [^abc] позволяет выбрать что-то, что не a,не b и не c\n",
    "- \\s - выражение для знака пробела\n",
    "- `+` это один или более повторов предыдущего символа. Например, \\s+ - это двойные (и больше) пробелы.\n",
    "\n",
    "Таким образом, `[^a-zA-Z\\s]` выбирает из текста всё, что не аглийские буквы в любом кейсе и пробелы. Т.е. пробелы и что-то, что не распознаётся, как буквы английского языка, например, кирилица или греческие буквы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbc6cf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Antarctic Muon And Neutrino Detector Array AMANDA is a highenergy neutrino telescope operating at the geographic South Pole It is a lattice of photomultiplier tubes buried deep in the polar ice between m and m The primary goal of this detector is to discover astrophysical sources of high energy neutrinos A highenergy muon neutrino coming through the earth from the Northern Hemisphere can be identified by the secondary muon moving upward through the detector The muon tracks are reconstructed with a maximum likelihood method It models the arrival times and amplitudes of Cherenkov photons registered by the photomultipliers This paper describes the different methods of reconstruction which have been successfully implemented within AMANDA Strategies for optimizing the reconstruction performance and rejecting background are presented For a typical analysis procedure the direction of tracks are reconstructed with about degree accuracy\n",
      "The Antarctic Muon And Neutrino Detector Array AMANDA is a highenergy neutrino telescope operating at the geographic South Pole It is a lattice of photomultiplier tubes buried deep in the polar ice between m and m The primary goal of this detector is to discover astrophysical sources of high energy neutrinos A highenergy muon neutrino coming through the earth from the Northern Hemisphere can be identified by the secondary muon moving upward through the detector The muon tracks are reconstructed with a maximum likelihood method It models the arrival times and amplitudes of Cherenkov photons registered by the photomultipliers This paper describes the different methods of reconstruction which have been successfully implemented within AMANDA Strategies for optimizing the reconstruction performance and rejecting background are presented For a typical analysis procedure the direction of tracks are reconstructed with about degree accuracy\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', texts[0])   #избавились от цифр и \"лишних\" символов вроде греческих букв\n",
    "cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  #избавились от двойных пробелов\n",
    "print(cleaned_text)   #печатаем очищенный абстракт\n",
    "\n",
    "#то же самое для заголовков\n",
    "cleaned_title = re.sub(r'[^a-zA-Z\\s]', '', titles[0])   #избавились от цифр и \"лишних\" символов вроде греческих букв\n",
    "cleaned_title = re.sub(r'\\s+', ' ', cleaned_text)  #избавились от двойных пробелов\n",
    "print(cleaned_title)   #печатаем очищенный абстракт"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637fddca",
   "metadata": {},
   "source": [
    "## Step 2: Tokenization with SpaCy and NLTK\n",
    "\n",
    "1. Load as nlp en_core_web_sm is a small English pipeline trained on written web text (blogs, news, comments), that includes vocabulary, syntax and entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91059d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token.text: The, token.pos_: DET, token.dep_: det\n",
      "token.text: Antarctic, token.pos_: PROPN, token.dep_: nmod\n",
      "token.text: Muon, token.pos_: PROPN, token.dep_: nmod\n",
      "token.text: And, token.pos_: CCONJ, token.dep_: cc\n",
      "token.text: Neutrino, token.pos_: PROPN, token.dep_: conj\n",
      "token.text: Detector, token.pos_: PROPN, token.dep_: compound\n",
      "token.text: Array, token.pos_: PROPN, token.dep_: nsubj\n",
      "token.text: (, token.pos_: PUNCT, token.dep_: punct\n",
      "token.text: AMANDA, token.pos_: PROPN, token.dep_: appos\n",
      "token.text: ), token.pos_: PUNCT, token.dep_: punct\n",
      "token.text: is, token.pos_: AUX, token.dep_: ROOT\n",
      "token.text: a, token.pos_: DET, token.dep_: det\n",
      "token.text: high, token.pos_: ADJ, token.dep_: amod\n",
      "token.text: -, token.pos_: PUNCT, token.dep_: punct\n",
      "token.text: energy, token.pos_: NOUN, token.dep_: nmod\n",
      "token.text: \n",
      ", token.pos_: SPACE, token.dep_: dep\n",
      "token.text: neutrino, token.pos_: NOUN, token.dep_: compound\n",
      "token.text: telescope, token.pos_: NOUN, token.dep_: attr\n",
      "token.text: operating, token.pos_: VERB, token.dep_: acl\n",
      "token.text: at, token.pos_: ADP, token.dep_: prep\n",
      "token.text: the, token.pos_: DET, token.dep_: det\n",
      "token.text: geographic, token.pos_: ADJ, token.dep_: amod\n",
      "token.text: South, token.pos_: PROPN, token.dep_: compound\n",
      "token.text: Pole, token.pos_: PROPN, token.dep_: pobj\n",
      "token.text: ., token.pos_: PUNCT, token.dep_: punct\n",
      "token.text: It, token.pos_: PRON, token.dep_: nsubj\n",
      "token.text: is, token.pos_: AUX, token.dep_: ROOT\n",
      "token.text: a, token.pos_: DET, token.dep_: det\n",
      "token.text: lattice, token.pos_: NOUN, token.dep_: attr\n",
      "token.text: of, token.pos_: ADP, token.dep_: prep\n",
      "token.text: \n",
      ", token.pos_: SPACE, token.dep_: dep\n",
      "token.text: photo, token.pos_: NOUN, token.dep_: npadvmod\n",
      "token.text: -, token.pos_: PUNCT, token.dep_: punct\n",
      "token.text: multiplier, token.pos_: ADJ, token.dep_: amod\n",
      "token.text: tubes, token.pos_: NOUN, token.dep_: pobj\n",
      "token.text: buried, token.pos_: VERB, token.dep_: acl\n",
      "token.text: deep, token.pos_: ADV, token.dep_: advmod\n",
      "token.text: in, token.pos_: ADP, token.dep_: prep\n",
      "token.text: the, token.pos_: DET, token.dep_: det\n",
      "token.text: polar, token.pos_: ADJ, token.dep_: amod\n",
      "token.text: ice, token.pos_: NOUN, token.dep_: pobj\n",
      "token.text: between, token.pos_: ADP, token.dep_: prep\n",
      "token.text: 1500, token.pos_: NUM, token.dep_: nummod\n",
      "token.text: m, token.pos_: NOUN, token.dep_: pobj\n",
      "token.text: and, token.pos_: CCONJ, token.dep_: cc\n",
      "token.text: 2000, token.pos_: NUM, token.dep_: nummod\n",
      "token.text: m., token.pos_: NOUN, token.dep_: conj\n",
      "token.text: \n",
      ", token.pos_: SPACE, token.dep_: dep\n",
      "token.text: The, token.pos_: DET, token.dep_: det\n",
      "token.text: primary, token.pos_: ADJ, token.dep_: amod\n",
      "token.text: goal, token.pos_: NOUN, token.dep_: nsubj\n",
      "token.text: of, token.pos_: ADP, token.dep_: prep\n",
      "token.text: this, token.pos_: DET, token.dep_: det\n",
      "token.text: detector, token.pos_: NOUN, token.dep_: pobj\n",
      "token.text: is, token.pos_: AUX, token.dep_: conj\n",
      "token.text: to, token.pos_: PART, token.dep_: aux\n",
      "token.text: discover, token.pos_: VERB, token.dep_: xcomp\n",
      "token.text: astrophysical, token.pos_: ADJ, token.dep_: amod\n",
      "token.text: sources, token.pos_: NOUN, token.dep_: dobj\n",
      "token.text: of, token.pos_: ADP, token.dep_: prep\n",
      "token.text: high, token.pos_: ADJ, token.dep_: amod\n",
      "token.text: \n",
      ", token.pos_: SPACE, token.dep_: dep\n",
      "token.text: energy, token.pos_: NOUN, token.dep_: compound\n",
      "token.text: neutrinos, token.pos_: NOUN, token.dep_: pobj\n",
      "token.text: ., token.pos_: PUNCT, token.dep_: punct\n",
      "token.text: A, token.pos_: DET, token.dep_: det\n",
      "token.text: high, token.pos_: ADJ, token.dep_: amod\n",
      "token.text: -, token.pos_: PUNCT, token.dep_: punct\n",
      "token.text: energy, token.pos_: NOUN, token.dep_: compound\n",
      "token.text: muon, token.pos_: NOUN, token.dep_: compound\n",
      "token.text: neutrino, token.pos_: NOUN, token.dep_: nsubjpass\n",
      "token.text: coming, token.pos_: VERB, token.dep_: acl\n",
      "token.text: through, token.pos_: ADP, token.dep_: prep\n",
      "token.text: the, token.pos_: DET, token.dep_: det\n",
      "token.text: earth, token.pos_: NOUN, token.dep_: pobj\n",
      "token.text: from, token.pos_: ADP, token.dep_: prep\n",
      "token.text: the, token.pos_: DET, token.dep_: det\n",
      "token.text: \n",
      ", token.pos_: SPACE, token.dep_: dep\n",
      "token.text: Northern, token.pos_: PROPN, token.dep_: compound\n",
      "token.text: Hemisphere, token.pos_: PROPN, token.dep_: pobj\n",
      "token.text: can, token.pos_: AUX, token.dep_: aux\n",
      "token.text: be, token.pos_: AUX, token.dep_: auxpass\n",
      "token.text: identified, token.pos_: VERB, token.dep_: ROOT\n",
      "token.text: by, token.pos_: ADP, token.dep_: agent\n",
      "token.text: the, token.pos_: DET, token.dep_: det\n",
      "token.text: secondary, token.pos_: ADJ, token.dep_: amod\n",
      "token.text: muon, token.pos_: NOUN, token.dep_: pobj\n",
      "token.text: moving, token.pos_: VERB, token.dep_: acl\n",
      "token.text: upward, token.pos_: ADV, token.dep_: advmod\n",
      "token.text: \n",
      ", token.pos_: SPACE, token.dep_: dep\n",
      "token.text: through, token.pos_: ADP, token.dep_: prep\n",
      "token.text: the, token.pos_: DET, token.dep_: det\n",
      "token.text: detector, token.pos_: NOUN, token.dep_: pobj\n",
      "token.text: ., token.pos_: PUNCT, token.dep_: punct\n",
      "token.text: The, token.pos_: DET, token.dep_: det\n",
      "token.text: muon, token.pos_: NOUN, token.dep_: compound\n",
      "token.text: tracks, token.pos_: NOUN, token.dep_: nsubjpass\n",
      "token.text: are, token.pos_: AUX, token.dep_: auxpass\n",
      "token.text: reconstructed, token.pos_: VERB, token.dep_: ROOT\n",
      "token.text: with, token.pos_: ADP, token.dep_: prep\n",
      "token.text: a, token.pos_: DET, token.dep_: det\n",
      "token.text: maximum, token.pos_: ADJ, token.dep_: amod\n",
      "token.text: \n",
      ", token.pos_: SPACE, token.dep_: dep\n",
      "token.text: likelihood, token.pos_: NOUN, token.dep_: compound\n",
      "token.text: method, token.pos_: NOUN, token.dep_: pobj\n",
      "token.text: ., token.pos_: PUNCT, token.dep_: punct\n",
      "token.text: It, token.pos_: PRON, token.dep_: nsubj\n",
      "token.text: models, token.pos_: VERB, token.dep_: ROOT\n",
      "token.text: the, token.pos_: DET, token.dep_: det\n",
      "token.text: arrival, token.pos_: NOUN, token.dep_: compound\n",
      "token.text: times, token.pos_: NOUN, token.dep_: dobj\n",
      "token.text: and, token.pos_: CCONJ, token.dep_: cc\n",
      "token.text: amplitudes, token.pos_: NOUN, token.dep_: conj\n",
      "token.text: of, token.pos_: ADP, token.dep_: prep\n",
      "token.text: Cherenkov, token.pos_: PROPN, token.dep_: compound\n",
      "token.text: \n",
      ", token.pos_: SPACE, token.dep_: dep\n",
      "token.text: photons, token.pos_: NOUN, token.dep_: pobj\n",
      "token.text: registered, token.pos_: VERB, token.dep_: acl\n",
      "token.text: by, token.pos_: ADP, token.dep_: agent\n",
      "token.text: the, token.pos_: DET, token.dep_: det\n",
      "token.text: photo, token.pos_: NOUN, token.dep_: compound\n",
      "token.text: -, token.pos_: PUNCT, token.dep_: punct\n",
      "token.text: multipliers, token.pos_: NOUN, token.dep_: pobj\n",
      "token.text: ., token.pos_: PUNCT, token.dep_: punct\n",
      "token.text: This, token.pos_: DET, token.dep_: det\n",
      "token.text: paper, token.pos_: NOUN, token.dep_: nsubj\n",
      "token.text: describes, token.pos_: VERB, token.dep_: ROOT\n",
      "token.text: the, token.pos_: DET, token.dep_: det\n",
      "token.text: different, token.pos_: ADJ, token.dep_: amod\n",
      "token.text: \n",
      ", token.pos_: SPACE, token.dep_: dep\n",
      "token.text: methods, token.pos_: NOUN, token.dep_: dobj\n",
      "token.text: of, token.pos_: ADP, token.dep_: prep\n",
      "token.text: reconstruction, token.pos_: NOUN, token.dep_: pobj\n",
      "token.text: ,, token.pos_: PUNCT, token.dep_: punct\n",
      "token.text: which, token.pos_: PRON, token.dep_: nsubjpass\n",
      "token.text: have, token.pos_: AUX, token.dep_: aux\n",
      "token.text: been, token.pos_: AUX, token.dep_: auxpass\n",
      "token.text: successfully, token.pos_: ADV, token.dep_: advmod\n",
      "token.text: implemented, token.pos_: VERB, token.dep_: relcl\n",
      "token.text: within, token.pos_: ADP, token.dep_: prep\n",
      "token.text: \n",
      ", token.pos_: SPACE, token.dep_: dep\n",
      "token.text: AMANDA, token.pos_: PROPN, token.dep_: pobj\n",
      "token.text: ., token.pos_: PUNCT, token.dep_: punct\n",
      "token.text: Strategies, token.pos_: NOUN, token.dep_: nsubjpass\n",
      "token.text: for, token.pos_: ADP, token.dep_: prep\n",
      "token.text: optimizing, token.pos_: VERB, token.dep_: pcomp\n",
      "token.text: the, token.pos_: DET, token.dep_: det\n",
      "token.text: reconstruction, token.pos_: NOUN, token.dep_: compound\n",
      "token.text: performance, token.pos_: NOUN, token.dep_: dobj\n",
      "token.text: and, token.pos_: CCONJ, token.dep_: cc\n",
      "token.text: rejecting, token.pos_: VERB, token.dep_: conj\n",
      "token.text: \n",
      ", token.pos_: SPACE, token.dep_: dep\n",
      "token.text: background, token.pos_: NOUN, token.dep_: dobj\n",
      "token.text: are, token.pos_: AUX, token.dep_: auxpass\n",
      "token.text: presented, token.pos_: VERB, token.dep_: ROOT\n",
      "token.text: ., token.pos_: PUNCT, token.dep_: punct\n",
      "token.text: For, token.pos_: ADP, token.dep_: prep\n",
      "token.text: a, token.pos_: DET, token.dep_: det\n",
      "token.text: typical, token.pos_: ADJ, token.dep_: amod\n",
      "token.text: analysis, token.pos_: NOUN, token.dep_: compound\n",
      "token.text: procedure, token.pos_: NOUN, token.dep_: pobj\n",
      "token.text: the, token.pos_: DET, token.dep_: det\n",
      "token.text: direction, token.pos_: NOUN, token.dep_: nsubjpass\n",
      "token.text: of, token.pos_: ADP, token.dep_: prep\n",
      "token.text: \n",
      ", token.pos_: SPACE, token.dep_: dep\n",
      "token.text: tracks, token.pos_: NOUN, token.dep_: pobj\n",
      "token.text: are, token.pos_: AUX, token.dep_: auxpass\n",
      "token.text: reconstructed, token.pos_: VERB, token.dep_: ROOT\n",
      "token.text: with, token.pos_: ADP, token.dep_: prep\n",
      "token.text: about, token.pos_: ADV, token.dep_: advmod\n",
      "token.text: 2, token.pos_: NUM, token.dep_: nummod\n",
      "token.text: degree, token.pos_: NOUN, token.dep_: compound\n",
      "token.text: accuracy, token.pos_: NOUN, token.dep_: pobj\n",
      "token.text: ., token.pos_: PUNCT, token.dep_: punct\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")  # load \n",
    "doc = nlp(texts[0])   #use nlp to retrive tokens from the text[0] (our first abstract)\n",
    "for token in doc:     #print the tokens\n",
    "    str0 = \"token.text: {}, token.pos_: {}, token.dep_: {}\".format(token.text, token.pos_, token.dep_)\n",
    "    print(str0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18250092",
   "metadata": {},
   "source": [
    "Сразу видно первую проблему - в тексте очень много специфических аббривеаутур, которые человеку (и, наверное, машине) без чтения контекста непонятны."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ed86ac",
   "metadata": {},
   "source": [
    "Выполним то же самое с помощью NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2791fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Antarctic', 'Muon', 'And', 'Neutrino', 'Detector', 'Array', '(', 'AMANDA', ')', 'is', 'a', 'high-energy', 'neutrino', 'telescope', 'operating', 'at', 'the', 'geographic', 'South', 'Pole', '.', 'It', 'is', 'a', 'lattice', 'of', 'photo-multiplier', 'tubes', 'buried', 'deep', 'in', 'the', 'polar', 'ice', 'between', '1500m', 'and', '2000m', '.', 'The', 'primary', 'goal', 'of', 'this', 'detector', 'is', 'to', 'discover', 'astrophysical', 'sources', 'of', 'high', 'energy', 'neutrinos', '.', 'A', 'high-energy', 'muon', 'neutrino', 'coming', 'through', 'the', 'earth', 'from', 'the', 'Northern', 'Hemisphere', 'can', 'be', 'identified', 'by', 'the', 'secondary', 'muon', 'moving', 'upward', 'through', 'the', 'detector', '.', 'The', 'muon', 'tracks', 'are', 'reconstructed', 'with', 'a', 'maximum', 'likelihood', 'method', '.', 'It', 'models', 'the', 'arrival', 'times', 'and', 'amplitudes', 'of', 'Cherenkov', 'photons', 'registered', 'by', 'the', 'photo-multipliers', '.', 'This', 'paper', 'describes', 'the', 'different', 'methods', 'of', 'reconstruction', ',', 'which', 'have', 'been', 'successfully', 'implemented', 'within', 'AMANDA', '.', 'Strategies', 'for', 'optimizing', 'the', 'reconstruction', 'performance', 'and', 'rejecting', 'background', 'are', 'presented', '.', 'For', 'a', 'typical', 'analysis', 'procedure', 'the', 'direction', 'of', 'tracks', 'are', 'reconstructed', 'with', 'about', '2', 'degree', 'accuracy', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/victoria/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "tokens_nltk = word_tokenize(texts[0])\n",
    "# print(tokens_nltk[:20])\n",
    "print(tokens_nltk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64fd14d",
   "metadata": {},
   "source": [
    "## 3: Stopwords removal\n",
    "\n",
    "Filter out common words that are not meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "041ff4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'about', 'at', 'by', 'through', 'This', 'been', 'are', 'for', 'and', 'with', 'which', 'this', 'It', 'from', 'have', 'to', 'is', 'can', 'within', 'The', 'For', 'And', 'in', 'a', 'be', 'A', 'of', 'between']\n"
     ]
    }
   ],
   "source": [
    "#SpaCy version\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Extract tokens\n",
    "tokens = [token.text for token in doc]\n",
    "tokens_filtered = [t for t in tokens if t.lower() not in STOP_WORDS]\n",
    "tokens_gone = list(set(tokens) - set(tokens_filtered))\n",
    "# print(tokens_filtered[:20])\n",
    "print(tokens_gone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "272a22aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'about', 'at', 'by', 'through', 'This', 'been', 'are', 'for', 'and', 'with', 'which', 'this', 'It', 'from', 'have', 'to', 'is', 'can', 'The', 'For', 'And', 'in', 'a', 'be', 'A', 'of', 'between']\n"
     ]
    }
   ],
   "source": [
    "#NLTK version\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens_filtered_nltk = [t for t in tokens_nltk if t.lower() not in stop_words]  #значимые слова, которые остались\n",
    "tokens_gone = list(set(tokens_nltk) - set(tokens_filtered_nltk))  #\"общие\" слова, которые мы отфильтровали\n",
    "\n",
    "print(tokens_gone)\n",
    "# print(tokens_filtered_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ddcb4",
   "metadata": {},
   "source": [
    "Интересное: spacy отфильтровала больше стоп-слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100720d3",
   "metadata": {},
   "source": [
    "## 4. Lemmatization\n",
    "\n",
    "Convert words to base form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f392716d",
   "metadata": {},
   "source": [
    "### With SpaCy\n",
    "В случае SpaCy это делается в две строки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8a76d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'Antarctic', 'Muon', 'and', 'Neutrino', 'Detector', 'Array', 'AMANDA', 'be', 'a', 'high', 'energy', 'neutrino', 'telescope', 'operate', 'at', 'the', 'geographic', 'South', 'Pole']\n"
     ]
    }
   ],
   "source": [
    "# with SpaCy\n",
    "lemmas = [token.lemma_ for token in doc if token.is_alpha]\n",
    "print(lemmas[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bea8b7f",
   "metadata": {},
   "source": [
    "### With NLTK\n",
    "В случае с NLTK это более заморочно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "066e4247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/victoria/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/victoria/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/victoria/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# импорт нужных библиотек\n",
    "from nltk.stem import WordNetLemmatizer  #reduces words to their base forms\n",
    "from nltk.corpus import wordnet  #WordNet это лексическая база данных, с помощью которой мы будет определять \n",
    "## какой частью речи является слово при лемматизации \n",
    "\n",
    "# Download necessary resources (only once)\n",
    "nltk.download('wordnet')  #загружает WordNet БД для лемматизации\n",
    "nltk.download('omw-1.4')  #multilingual WordNet для поддержки разных языков (а оно нам надо? У нас только английский так-то)\n",
    "nltk.download('averaged_perceptron_tagger_eng') #подгружаем части речи в английском языке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c71c36f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Antarctic', 'Muon', 'Neutrino', 'Detector', 'Array', '(', 'AMANDA', ')', 'high-energy', 'neutrino', 'telescope', 'operate', 'geographic', 'South', 'Pole', '.', 'lattice', 'photo-multiplier', 'tube', 'bury', 'deep', 'polar', 'ice', '1500m', '2000m', '.', 'primary', 'goal', 'detector', 'discover', 'astrophysical', 'source', 'high', 'energy', 'neutrino', '.', 'high-energy', 'muon', 'neutrino', 'come', 'earth', 'Northern', 'Hemisphere', 'identify', 'secondary', 'muon', 'move', 'upward', 'detector', '.', 'muon', 'track', 'reconstruct', 'maximum', 'likelihood', 'method', '.', 'model', 'arrival', 'time', 'amplitudes', 'Cherenkov', 'photon', 'register', 'photo-multipliers', '.', 'paper', 'describe', 'different', 'method', 'reconstruction', ',', 'successfully', 'implement', 'within', 'AMANDA', '.', 'Strategies', 'optimize', 'reconstruction', 'performance', 'reject', 'background', 'present', '.', 'typical', 'analysis', 'procedure', 'direction', 'track', 'reconstruct', '2', 'degree', 'accuracy', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer() #заводим инстанс лемматировщика - он переводит слова в леммы, их базовые формы\n",
    "\n",
    "# Convert POS tags to WordNet format (important for accuracy)\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default\n",
    "\n",
    "# Lemmatize each token with its POS tag\n",
    "pos_tags = nltk.pos_tag(tokens_filtered_nltk)    #берём писок токенов из нашего первого абстракта\n",
    "lemmas = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7b0c2b",
   "metadata": {},
   "source": [
    "Внутри функции get_wordnet_pos мы мапим NLTK POS тэги с чем-то типа тэгов из WordNet. А именно, в WordNet всего 4 типа часте речи: существительное, глагол, приланательное, наречие. А nltk поддерживает много тэгов, которые являются подтипами этих штук: например, VBD - это глагол в прошедгем времени. Чтобы замаппить одно с другим, делается следующее:\n",
    "- Прилагательные: если тэг nltk начинается на J, эта часть речи соответствует wordnet.ADJ\n",
    "- Глаголы: если тэг nltk начинается на V, эта часть речи соответствует  wordnet.VERB\n",
    "- Существительные: если тэг nltk начинается на N, эта часть речи соответствует wordnet.NOUN\n",
    "- Наречия: если тэг nltk начинается на R, эта часть речи соответствует wordnet.ADV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d78492",
   "metadata": {},
   "source": [
    "## Обработка всех абстрактов и их заголовков\n",
    "С использованием Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "875be739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# попробуем опрделить функцию для очистки и токенизации текстовых данных\n",
    "def clean_and_lemmatize(texts):\n",
    "    #функция принимает список текстов, чистит из ото всякого лишнего, разбивает на токены и возвращает эти токены обратно\n",
    "    all_tokens = []  #здесь будут токены всех текстов\n",
    "\n",
    "    for text in texts:  #для каждого абстракта\n",
    "        #чистим текст:\n",
    "        cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)   #избавились от цифр и \"лишних\" символов вроде греческих букв\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  #избавились от двойных пробелов\n",
    "        #определяеем токены\n",
    "        doc = nlp(text)   #делаем из текста spaCy-объект\n",
    "        tokens = []      #список токенов абстракта\n",
    "        for token in doc:\n",
    "            # Оставляем только слова (без цифр и знаков препинания)\n",
    "            if token.is_alpha:       #token.is_alpha True, если токен состоит только из букв, иначе False\n",
    "                # Игнорируем стоп-слова\n",
    "                if token.text.lower() not in STOP_WORDS:   #ставим слово в lower case и проверяем, что оно не стоп-слово\n",
    "                    tokens.append(token.lemma_)     #добавляем базовую форму (aka лемму) в список токенов абстракта\n",
    "        all_tokens.append(tokens)    # Добавляем список токенов для этого текста в общий список токенов всех 1000 абстрактов\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44aba74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#код выше можно записать в одну строку вот так, но тогда сложно будет быстро вспомнить, что здесь происходит\n",
    "# df['tokens'] = [ [token.lemma_ for token in nlp(text) if token.is_alpha and token.text.lower() not in STOP_WORDS] for text in texts ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5138ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# чистим абстракты и сохраняем результат в колонку 'tokens' датафрейма\n",
    "df['summary_tokens'] = clean_and_lemmatize(texts)\n",
    "# проверка работы\n",
    "print(df.summary_tokens[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281a132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# делаем то же самое для заголовков:\n",
    "df['title_tokens'] = clean_and_lemmatize(titles)\n",
    "# проверка работы\n",
    "print(df.title_tokens[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8bab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем итоговый общий столбец с токенами\n",
    "df['tokens_combined'] = df['title_tokens'] + df['summary_tokens']\n",
    "# проверка работы:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a87134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.tokens_combined[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c3269d",
   "metadata": {},
   "source": [
    "## Сохранение результатов на диск\n",
    "Пока что в csv, но в будущем будем их писать в MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639f8151",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"preprocessed_abstracts.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_local)",
   "language": "python",
   "name": "nlp_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
